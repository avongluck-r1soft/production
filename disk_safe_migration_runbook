
Login to wdcpxe01 - this is where dsMigrationSupreme should run.

Open an xterminal, login to wdcpxe01. IP address is 10.125.31.86.

Open an xterminal for the source CSBM, where the diskSafe will be migrating FROM.

    Example Greylog alert for storage filling up:

        Last messages accounting for this alert:
source: wdcsbm13 | message: DEVOPS -- WARNING wdcsbm13.itsupport247.net (169.45.192.171) DiskUsage high = 92 mount_point = /storage05 { level: 5 | gl2_remote_ip: 10.148.215.86 | gl2_remote_port: 46952 | streams: [56815348b3a2f2be38cba018] | alert_value: 92 | gl2_source_input: 5672f7e5b3a2f2d7c07bb9d5 | application_name: continuum | gl2_source_node: d2636a89-c572-4717-b7e8-f7cada1266c6 | _id: af203e01-2e86-11e6-a907-0cc47abce8b5 | facility: user-level | timestamp: 2016-06-09T21:11:08.003Z }

    So for this alert, we see that the CSBM hostname is wdcsbm13, disk usage is 92%, mount point is /storage05.

    sudo to the root account

    cd /storage05/replication

    du -sh *

    Find a diskSafe UUID that is in or around the 1-3 TB size range.

    Run the following command to make sure that the UUID is legitimate: getDiskSafe <UUID>

Go back to wdcpxe01 *** THIS IS WHERE dsMigrationSupreme RUNS IN PRODUCTION ***

    Login as continuum

    Run dsMigrationSupreme to move a diskSafe, using the agent UUID, from a source SBM to a destination SBM, using
source and destination UUIDs to identify the source & destination of the diskSafe that you're moving. 

    Run dsMigrationSupreme once to check out what the command line arguments are:

        continuum@wdcpxe01:~$ ./dsMigrationSupreme 
            Usage: dsMigrateSupreme <agentUUID> <source-SBM-UUID> <dest-SBM-UUID>

    Sample run:

    continuum@wdcpxe01:~$ ./dsMigrationSupreme 06662b6f-0c21-428b-ba5d-ce8c9a6695bf 1b8c1419-e899-4061-9bd3-9168586a24c3  be96c92e-3716-433c-90e7-520857473e18

    

